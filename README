This is a library for computing entropy and other information-theory-related functions on data. Currently it will find the entropy of a file or stdin in bits, nats, or digits over the file's bytes. More options on their way; still far from an actual release.

Also, this is absurdly non-optimized as of now; I'm working on making sure it's correct at the expense of higher memory usage and slower i/o.

make entropy will produce a binary called "ith". Its options are

-x print the Pearson chi-squared against the null hypothesis of uniform distribution (default is to print the entropy)
-a 2 | 8 | 16 | 32 | 64 | char | wchar | word | wword (width of alphabet)
-u binary | decimal | natural (compute information in bits, digits, and nats respectively)
-f file (default reads from stdin)
-p print the probability distribution of the sample in addition to the entropy

entropy is one driver for what I hope will end up being a more general shared library.