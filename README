This is a library for computing entropy and other information-theory-related functions on data. Currently it will find the entropy of a file or stdin in bits, nats, or digits over the file's bytes. More options on their way; still far from an actual release.

Also, this is absurdly non-optimized as of now; I'm working on making sure it's correct at the expense of higher memory usage and slower i/o.

make entropy will produce a binary called "entropy". Its options are

-a 2 | 8 | 16 | 32 | 64 | char  (width of alphabet; char is wchar_t)
-u binary | decimal | natural (compute in bits, digits, and nats respectively)
-f file (default reads from stdin)
-p print the probability distribution of the sample in addition to the entropy

entropy is one driver for what I hope will end up being a more general shared library.